{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"model_7.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"o_VZnTokaE9I","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598814915655,"user_tz":-60,"elapsed":1224,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import numpy as np\n","import pandas as pd\n","from itertools import cycle\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","file_path = './cnn_stories_tokenized'\n","gdd.download_file_from_google_drive(file_id='0BzQ6rtO2VN95cmNuc2xwUS1wdEE', \n","                                    dest_path=file_path+'.zip', \n","                                    unzip=True, \n","                                    showsize=True)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKpwsh1Ylyf5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1598814978563,"user_tz":-60,"elapsed":64120,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"3faea799-dda3-4a38-b5bf-3345d7b280c3"},"source":["from os import listdir\n","from tqdm import tqdm\n","\n","def load_data(file_path):\n","    data = []\n","    file_list = listdir(file_path)\n","    for name in tqdm(file_list):\n","        file_name = file_path + '/' + name\n","        doc = load_doc(file_name)\n","        src, trg = split_doc(doc)\n","        data.append({'src': src, 'trg': trg})\n","    return data\n","\n","def load_doc(file_name):\n","    file = open(file_name, encoding='utf-8')\n","    text = file.read()\n","    file.close()\n","    return text\n","\n","def split_doc(doc):\n","    idx = doc.find('@highlight')\n","    src, trg = doc[:idx], doc[idx:].split('@highlight')\n","    trg = [t.strip() for t in trg if len(t) > 0]\n","    return src, trg\n","\n","def clean_sent(sent):\n","    filter_list = ['/', '-LRB-', '-RRB-', '\\n', '`', '\\'\\'', '\"', '--', '...', 'NEW :']\n","    for token in filter_list:\n","        sent = sent.replace(token, ' ')\n","    sent = ' '.join(sent.split())\n","    sent = sent.lower()\n","    return sent\n","\n","def preprocess(file_path, train_size, valid_size, test_size, max_enc_step, max_dec_step):\n","    # load data from file_path\n","    data = load_data(file_path)\n","    preprocessed = []\n","    for ex in data:\n","        src, trg = ex['src'], ex['trg']\n","        src = clean_sent(src)\n","        trg = [clean_sent(t) for t in trg]\n","        # choose the first hightlight as the truth\n","        # choose the first 2 highlights as the trg\n","        if len(trg) > 1:\n","            trg = trg[:2]\n","            trg = ' . '.join(trg)\n","        else:\n","            trg = trg[0]\n","        trg += ' .'\n","        # truncate examples\n","        if len(src) > max_enc_step > 0: \n","            src = src.split()\n","            src = src[:max_enc_step]\n","            src = ' '.join(src)\n","        if len(trg) > max_dec_step > 0: \n","            trg = trg.split()\n","            trg = trg[:max_dec_step]\n","            trg = ' '.join(trg)\n","        preprocessed.append({'src': src, 'trg': trg})\n","    # split data\n","    train_data = preprocessed[:train_size]\n","    valid_data = preprocessed[train_size:train_size+valid_size]\n","    test_data = preprocessed[-test_size:]\n","    print('Number of train examples: {}'.format(len(train_data)))\n","    print('Number of valid examples: {}'.format(len(valid_data)))\n","    print('Number of test examples: {}'.format(len(test_data)))\n","    return train_data, valid_data, test_data\n","\n","\n","train_size = 90000\n","valid_size = 1579\n","test_size = 1000\n","max_enc_step = 300\n","max_dec_step = -1\n","train_data, valid_data, test_data = preprocess(file_path, \n","                                               train_size, \n","                                               valid_size, \n","                                               test_size,\n","                                               max_enc_step, \n","                                               max_dec_step)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|██████████| 92579/92579 [00:51<00:00, 1810.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Number of train examples: 90000\n","Number of valid examples: 1579\n","Number of test examples: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zVKkiNGIIArh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1598815007014,"user_tz":-60,"elapsed":92561,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"fea4b54e-bed2-452f-ac3d-0ccde7facfba"},"source":["class Vocab():\n","   \n","    def __init__(self, data, max_size=80000, min_freq=4):\n","        self.PAD = 0\n","        self.SOS = 1\n","        self.EOS = 2\n","        self.UNK = 3\n","        self.index2token = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n","        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n","        self.token2count = {}\n","        self.n_tokens = 4\n","        self.max_size = max_size\n","        self.min_freq = min_freq\n","\n","        print(\"Read {} example pairs\".format(len(data)))\n","        for ex in data:\n","            self.add_sent(ex['src'])\n","            self.add_sent(ex['trg'])\n","        print('Number of vocab: {}'.format(self.n_tokens))\n","\n","        vocab_count = list(self.token2count.items()) \n","        vocab_count = sorted(vocab_count, key=lambda x: x[1], reverse=True) # Sort by counts (descending)\n","        if max_size-4 < len(vocab_count):\n","            vocab_count = vocab_count[:max_size-4]\n","\n","        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n","        self.token2count = {}\n","        self.index2token = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.n_tokens = 4\n","\n","        for token, count in vocab_count:\n","            if count >= min_freq: \n","                self.token2index[token] = self.n_tokens\n","                self.token2count[token] = count\n","                self.index2token[self.n_tokens] = token\n","                self.n_tokens += 1    \n","        print('Number of vocab left: {}'.format(self.n_tokens))\n","\n","    def add_sent(self, sent):\n","        for token in sent.split():\n","            self.add_token(token)\n","\n","    def add_token(self, token):\n","        if sum([char.isdigit() for char in token]) == 0:\n","            if token not in self.token2index:\n","                self.token2index[token] = self.n_tokens\n","                self.token2count[token] = 1\n","                self.index2token[self.n_tokens] = token\n","                self.n_tokens += 1\n","            else:\n","                self.token2count[token] += 1\n","\n","    def __getitem__(self, item):\n","        if type(item) == str:\n","            return int(self.token2index.get(item, self.UNK)) # Return the index of <UNK> if input word is not in the vocab\n","        elif type(item) == int:\n","            return self.index2token[item]\n","\n","def build_oovs(src_tokens):\n","    oovs = [token for token in src_tokens if vocab[token] == vocab.UNK]\n","    return oovs\n","\n","def tokens2indices(tokens):\n","    indices = [vocab[token] for token in tokens]\n","    indices = [vocab.SOS] + indices + [vocab.EOS]\n","    return indices\n","\n","def extended2indices(extended, oovs):\n","    indices = []\n","    for token in extended:\n","        idx = vocab[token]\n","        if idx == vocab.UNK:\n","            idx = vocab.n_tokens + oovs.index(token) if token in oovs else vocab.UNK\n","        indices.append(idx)\n","    indices = [vocab.SOS] + indices + [vocab.EOS]\n","    return indices\n","\n","def indices2extended(indices, oovs):\n","    tokens = []\n","    for idx in indices:\n","        if idx > vocab.n_tokens-1:\n","            token = oovs[idx - vocab.n_tokens]\n","        else:\n","            token = vocab[idx]\n","        tokens.append(token)\n","    return tokens\n","\n","\n","vocab = Vocab(train_data)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Read 90000 example pairs\n","Number of vocab: 197195\n","Number of vocab left: 80000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q69Hn41vVRwd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598815007016,"user_tz":-60,"elapsed":92554,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["class Example():\n","    def __init__(self, ex, max_src_len=100):\n","\n","        self.src_sent = ex['src']\n","        self.trg_sent = ex['trg']\n","        self.src_tokens = self.src_sent.split() \n","        self.trg_tokens = self.trg_sent.split()\n","        self.src = tokens2indices(self.src_tokens) # For train\n","        self.trg = tokens2indices(self.trg_tokens)[:-1] # For train\n","        self.oovs = build_oovs(self.src_tokens)\n","        self.src_extended_vocab = extended2indices(self.src_tokens, self.oovs) # For attn dist\n","        self.trg_extended_vocab = extended2indices(self.trg_tokens, self.oovs)[1:] # For eval\n","        self.src_len = len(self.src)\n","        self.trg_len = len(self.trg)\n","\n","def pad_sent(sent, max_len):\n","    sent += [vocab.PAD]*(max_len - len(sent))\n","    return sent\n","\n","class Batch():\n","    def __init__(self, example_list):\n","        self.batch_size = len(example_list)\n","        self.original_src = [example.src_sent for example in example_list]\n","        self.original_trg = [example.trg_sent for example in example_list]\n","\n","        self.max_src_len = max([example.src_len for example in example_list])\n","        self.max_trg_len = max([example.trg_len for example in example_list])\n","\n","        self.src = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n","        self.src_mask = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n","        self.trg = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For loss calculation\n","        self.src_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n","        self.trg_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n","        self.src_extended_vocab = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For attn dist\n","        self.trg_extended_vocab = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For eval\n","        self.max_oovs_len = max([len(example.oovs) for example in example_list]) # For attn dist\n","        self.oovs = [example.oovs for example in example_list] # For attn dist\n","        \n","        for i, example in enumerate(example_list):\n","\n","            self.src[i] = pad_sent(example.src, self.max_src_len)\n","            self.src_mask[i] = pad_sent([1]*(example.src_len), self.max_src_len)\n","            self.trg[i] = pad_sent(example.trg, self.max_trg_len)\n","            self.src_len[i] = example.src_len \n","            self.trg_len[i] = example.trg_len \n","            self.src_extended_vocab[i, :] = pad_sent(example.src_extended_vocab, self.max_src_len)\n","            self.trg_extended_vocab[i, :] = pad_sent(example.trg_extended_vocab, self.max_trg_len)\n","\n","        self.src = torch.from_numpy(self.src).long().to(device)\n","        self.src_mask = torch.from_numpy(self.src_mask).long().to(device)\n","        self.trg = torch.from_numpy(self.trg).long().to(device)\n","        self.src_len = torch.from_numpy(self.src_len).long().to(device)\n","        self.trg_len = torch.from_numpy(self.trg_len).long().to(device)\n","        self.src_extended_vocab = torch.from_numpy(self.src_extended_vocab).long().to(device)\n","        self.trg_extended_vocab = torch.from_numpy(self.trg_extended_vocab).long().to(device)\n","        self.oov_pad = torch.zeros(self.batch_size, self.max_oovs_len).to(device) if self.max_oovs_len > 0 else None # [batch_size, oov_len]\n","\n","\n","def batchify(data, batch_size):\n","    examples = [Example(ex) for ex in data]\n","    examples = sorted(examples, key=lambda example: example.src_len, reverse=True)\n","    examples = [examples[i:i+batch_size] for i in range(0, len(examples), batch_size)]\n","    examples = [Batch(batch) for batch in examples]\n","    return examples\n","\n","class Dataset():\n","    def __init__(self, data, batch_size):\n","        self.batches = batchify(data, batch_size)\n","    \n","    def process_data(self):\n","        for batch in self.batches:\n","            yield batch\n","        \n","    def get_stream(self):\n","        return cycle(self.process_data())\n","\n","    def __iter__(self):\n","        return self.get_stream()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioC8dDUeZYia","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":386},"executionInfo":{"status":"ok","timestamp":1598815007754,"user_tz":-60,"elapsed":93283,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"c1d4b5d5-1444-468d-9d40-35f4d4e0fff7"},"source":["class Encoder(nn.Module):\n","    def __init__(self, emb_dim, hid_dim):\n","        super(Encoder, self).__init__()\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n","        self.reduce_hidden = nn.Linear(hid_dim*2, hid_dim, bias=False)\n","        self.reduce_cell = nn.Linear(hid_dim*2, hid_dim, bias=False)\n","    def forward(self, embedded, src_len): \n","        packed = pack_padded_sequence(embedded, src_len, batch_first=True) \n","        output, (hidden, cell) = self.lstm(packed)\n","        output, _ = pad_packed_sequence(output, batch_first=True)\n","        hidden = F.relu(self.reduce_hidden(torch.cat((hidden[-2, :, : ], hidden[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n","        cell = F.relu(self.reduce_cell(torch.cat((cell[-2, :, : ], cell[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n","        return output, hidden, cell\n","\n","class Attention(nn.Module):\n","    def __init__(self, hid_dim):\n","        super(Attention, self).__init__()\n","        self.W_h = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n","        self.W_s = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n","        self.w_c = nn.Linear(1, 2*hid_dim, bias=False)\n","        self.v = nn.Linear(2*hid_dim, 1, bias=False)\n","    def forward(self, output, hidden, cell, src_mask, coverage):\n","        hidden = torch.cat((hidden, cell), dim=2).permute(1, 0, 2) # [batch_size, src_len, 2*hid_dim]\n","        energy = self.v(torch.tanh(self.W_h(output) + self.W_s(hidden) + self.w_c(coverage.unsqueeze(2)))) # [batch_size, src_len, 2*hid_dim] EQ.11\n","        attn_dist = F.softmax(energy.squeeze(2), dim=1) # [batch_size, src_len]\n","        attn_dist = attn_dist * src_mask # [batch_size, src_len]\n","        attn_dist = attn_dist / attn_dist.sum(1, keepdim=True)\n","        context = torch.bmm(attn_dist.unsqueeze(1), output).squeeze(1) # [batch_size, 2*hid_dim]\n","        coverage = coverage.clone() + attn_dist # [batch_size, src_len] EQ.10\n","        return attn_dist, context, coverage\n","\n","class Pointer_Generator(nn.Module):\n","    def __init__(self, emb_dim, hid_dim, output_dim):\n","        super(Pointer_Generator, self).__init__()\n","        self.ptr = nn.Linear(emb_dim+4*hid_dim, 1, bias=True)\n","        self.V1 = nn.Linear(emb_dim+4*hid_dim, hid_dim, bias=True)\n","        self.V2 = nn.Linear(hid_dim, output_dim, bias=True)\n","    def forward(self, embedded, hidden, cell, context, attn_dist, src_extended_vocab, oov_pad):\n","\n","        hidden = torch.cat((hidden, cell), dim=2).squeeze(0) # [batch_size, 2*hid_dim]\n","\n","        # Generation probability\n","        gen_prob = torch.sigmoid(self.ptr(torch.cat((embedded, hidden, context), dim=1))) # [batch_size, 1]\n","\n","        # Vocabulary distribution \n","        vocab_dist = F.softmax(self.V2(self.V1(torch.cat((embedded, hidden, context), dim=1))), dim=1) # [batch_size, output_dim]\n","        vocab_dist = gen_prob * vocab_dist # [batch_size, output_dim+oov_len]\n","        if oov_pad != None:\n","            vocab_dist = torch.cat((vocab_dist, oov_pad), dim=1) # [batch_size, output_dim+oov_len]\n","            \n","        # Attention distribution\n","        attn_dist = (1-gen_prob) * attn_dist # [batch_size, src_len]\n","\n","        # Word distribution\n","        word_dist = vocab_dist.scatter_add(1, src_extended_vocab, attn_dist) # [batch_size, output_dim+oov_len]\n","        return word_dist\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim):\n","        super(Decoder, self).__init__()\n","        self.attention = Attention(hid_dim)\n","        self.lstm = nn.LSTM(emb_dim+2*hid_dim, hid_dim, bidirectional=False, batch_first=True)\n","        self.pointer_generator = Pointer_Generator(emb_dim, hid_dim, output_dim)\n","    def forward(self, src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad, coverage):\n","\n","        attn_dist, context, coverage = self.attention(output, hidden, cell, src_mask, coverage)\n","        decoder_input = torch.cat((embedded, context), dim=1).unsqueeze(1) # [batch_size, 1, emb_dim+2*hid_dim]\n","        _, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n","        word_dist = self.pointer_generator(embedded, hidden, cell, context, attn_dist, src_extended_vocab, oov_pad) # [batch_size, output_dim+oov_len]\n","        return word_dist, hidden, cell, attn_dist, coverage\n","\n","class Param():\n","    def __init__(self):\n","        self.vocab_dim = vocab.n_tokens\n","        self.emb_dim = 300\n","        self.hid_dim = 300\n","        self.lr = 0.001\n","        self.n_iters = 100000\n","        self.batch_size = 16\n","\n","class Model(nn.Module):\n","    def __init__(self, param):\n","        super(Model, self).__init__()\n","        self.embedding = nn.Embedding(param.vocab_dim, param.emb_dim).to(device)\n","        self.encoder = Encoder(param.emb_dim, param.hid_dim).to(device)\n","        self.decoder = Decoder(param.vocab_dim, param.emb_dim, param.hid_dim).to(device)\n","\n","param = Param()\n","model = Model(param)\n","model"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (embedding): Embedding(80000, 300)\n","  (encoder): Encoder(\n","    (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n","    (reduce_hidden): Linear(in_features=600, out_features=300, bias=False)\n","    (reduce_cell): Linear(in_features=600, out_features=300, bias=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (W_h): Linear(in_features=600, out_features=600, bias=False)\n","      (W_s): Linear(in_features=600, out_features=600, bias=False)\n","      (w_c): Linear(in_features=1, out_features=600, bias=False)\n","      (v): Linear(in_features=600, out_features=1, bias=False)\n","    )\n","    (lstm): LSTM(900, 300, batch_first=True)\n","    (pointer_generator): Pointer_Generator(\n","      (ptr): Linear(in_features=1500, out_features=1, bias=True)\n","      (V1): Linear(in_features=1500, out_features=300, bias=True)\n","      (V2): Linear(in_features=300, out_features=80000, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"BrRHEUROV0UN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598815007755,"user_tz":-60,"elapsed":93272,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["def calc_loss(vocab_dist, trg_step, attn_dist, coverage):\n","    probs = torch.gather(vocab_dist, 1, trg_step.unsqueeze(1)).squeeze()\n","    step_loss = -torch.log(probs + 1e-9) \n","    covloss = torch.sum(torch.min(attn_dist, coverage), dim=1) \n","    return step_loss + covloss\n","\n","def calc_rouge(refs, cans):\n","    rouge_1 = 0\n","    rouge_2 = 0\n","    rouge_3 = 0\n","    count = 0\n","    for ref, can in zip(refs, cans):\n","        assert type(ref) == type(can) == str\n","        ref = ref.split()\n","        can = can.split()\n","        \n","        rouge_1 += rouge_n([[ref]], [can], 1)\n","        rouge_2 += rouge_n([[ref]], [can], 2)\n","        rouge_3 += rouge_n([[ref]], [can], 3)\n","        count += 1\n","    \n","    rouge_1 = rouge_1*100 / count\n","    rouge_2 = rouge_2*100 / count\n","    rouge_3 = rouge_3*100 / count\n","\n","    return rouge_1, rouge_2, rouge_3\n","\n","def get_ngram(lst, n):\n","    ngram = [tuple(lst[i:i+n]) for i in range(len(lst) - n + 1)]\n","    return ngram\n","\n","def divide(numerator, denominator):\n","    if np.min(denominator) > 0:\n","        return numerator / denominator\n","    else:\n","        return 0.0\n","\n","def match(ref, sys):\n","    intersection = set(ref) & set(sys)\n","    return len(intersection)\n","\n","def single_rouge_n(ref, sys, N):\n","    ref_iterator = get_ngram(ref, N)\n","    sys_iterator = get_ngram(sys, N)\n","    match_count = match(ref_iterator, sys_iterator)\n","    rec_count = len(ref_iterator)\n","    \n","    return match_count, rec_count\n","    \n","def multi_rouge_n(refs, sys, N):\n","    temp_rouge = np.zeros([len(refs), 2])\n","    for i in range(len(refs)):\n","        temp_rouge[i, :] = single_rouge_n(refs[i], sys, N)\n","    rogue = divide(temp_rouge[:, 0], temp_rouge[:, 1])\n","    ind = np.argmax(rogue)\n","    \n","    match_count, rec_count = temp_rouge[ind, ]\n","    return match_count, rec_count\n","    \n","\n","def rouge_n(reference_summaries, candidate_summaries, N):\n","    # https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.XuWlF0VKhPa\n","    match_counts = 0.0\n","    rec_counts = 0.0\n","    \n","    for (refs, can) in zip(reference_summaries, candidate_summaries):\n","        match_count, rec_count = multi_rouge_n(refs, can, N)\n","        match_counts += match_count\n","        rec_counts += rec_count\n","        \n","    score = divide(match_counts, rec_counts)\n","    return score"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"WV3DVqnLfeRa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598815007756,"user_tz":-60,"elapsed":93263,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["def train_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    src_extended_vocab = batch.src_extended_vocab\n","    trg_extended_vocab = batch.trg_extended_vocab\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","    oov_pad = batch.oov_pad\n","    coverage = torch.zeros(src.size()).to(device)\n","\n","    embedded = model.embedding(src)\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","\n","    step_losses = []\n","    for t in range(trg.size(1)):\n","        embedded = model.embedding(trg[:, t]) # [batch_size, trg_len, emb_dim]\n","        word_dist, hidden, cell, attn_dist, coverage = model.decoder(src_mask,\n","                                                                     embedded, hidden, cell, output, \n","                                                                     src_extended_vocab, oov_pad,\n","                                                                     coverage)     \n","        step_loss = calc_loss(word_dist, trg_extended_vocab[:, t], coverage, attn_dist)\n","        step_losses.append(step_loss)\n","    step_losses = torch.sum(torch.stack(step_losses, dim=1), dim=1)\n","    batch_loss = torch.mean(step_losses/trg_len)\n","    return batch_loss\n","\n","def eval_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    src_extended_vocab = batch.src_extended_vocab\n","    trg_extended_vocab = batch.trg_extended_vocab\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","    oov_pad = batch.oov_pad\n","    coverage = torch.zeros(src.size()).to(device)\n","\n","    embedded = model.embedding(src)\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","\n","    step_losses = []\n","    for t in range(trg.size(1)):\n","        embedded = model.embedding(trg[:, t]) # [batch_size, trg_len, emb_dim]\n","        word_dist, hidden, cell, attn_dist, coverage = model.decoder(src_mask,\n","                                                                     embedded, hidden, cell, output, \n","                                                                     src_extended_vocab, oov_pad,\n","                                                                     coverage)        \n","        step_loss = calc_loss(word_dist, trg_extended_vocab[:, t], coverage, attn_dist)\n","        step_losses.append(step_loss)\n","    step_losses = torch.sum(torch.stack(step_losses, dim=1), dim=1)\n","    batch_loss = torch.mean(step_losses/trg_len)\n","    return batch_loss\n","\n","def eval_batches(batches, model):\n","    valid_losses = []\n","    with torch.no_grad():\n","        for batch in iter(batches):\n","            valid_loss = eval_batch(batch, model)\n","            valid_losses.append(valid_loss.item())\n","        return np.mean(valid_losses)\n","\n","def infer_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    src_extended_vocab = batch.src_extended_vocab\n","    trg_extended_vocab = batch.trg_extended_vocab\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","    oov_pad = batch.oov_pad\n","    coverage = torch.zeros(src.size()).to(device)\n","\n","    embedded = model.embedding(src) # [batch_size, src_len, emb_dim]\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","    pred_trgs = []\n","    pred_trg = torch.tensor([vocab.SOS]).to(device) # initial decoder input [batch_size]\n","    for t in range(100):\n","        embedded = model.embedding(pred_trg) # [batch_size, trg_len, emb_dim]\n","        word_dist, hidden, cell, attn_dist, coverage = model.decoder(src_mask,\n","                                                                     embedded, hidden, cell, output, \n","                                                                     src_extended_vocab, oov_pad,\n","                                                                     coverage)        \n","        pred_trg = word_dist.argmax(1) # [batch_size]\n","        pred_idx = pred_trg.item()\n","        pred_trgs.append(pred_idx)\n","        if pred_trgs[-1] == vocab.EOS:\n","            break\n","        elif pred_trgs[-1] > vocab.n_tokens-1:\n","            pred_trg = torch.tensor([vocab.UNK]).to(device)\n","    \n","    pred_trgs = indices2extended(pred_trgs, batch.oovs[0])\n","    pred_trgs = ' '.join(pred_trgs)\n","    return pred_trgs\n","\n","def infer_batches(batches, model, save=False):\n","    refs = []\n","    cans = []\n","    srcs = []\n","    with torch.no_grad():\n","        for batch in batches:\n","            src = batch.original_src[0]\n","            ref = batch.original_trg[0]\n","            can = infer_batch(batch, model)\n","            if can.split()[-1] == vocab[vocab.EOS]:\n","                can = can.split()\n","                can = can[:-1]\n","                can = ' '.join(can)\n","            refs.append(ref)\n","            cans.append(can)\n","            srcs.append(src)\n","    score, _, _ = calc_rouge(refs, cans)\n","    if save == True:\n","        df = pd.DataFrame({'src': srcs, 'reference': refs, 'candidate': cans})\n","        df.to_csv(path_or_buf='model_7.csv', index=False)\n","    return score"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZ5YAmKlE0hh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1598815007758,"user_tz":-60,"elapsed":93255,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"3e450c69-da20-4f45-f960-56c3439274d4"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'Embedding has {count_parameters(model.embedding):,} trainable parameters')\n","print(f'Encoder has {count_parameters(model.encoder):,} trainable parameters')\n","print(f'Decoder has {count_parameters(model.decoder):,} trainable parameters')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Embedding has 24,000,000 trainable parameters\n","Encoder has 1,804,800 trainable parameters\n","Decoder has 26,695,401 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xbGuJxHn-XQS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":858},"executionInfo":{"status":"error","timestamp":1598814901496,"user_tz":-60,"elapsed":6862580,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"50a42a70-4281-4e9b-9a6e-83ff67431f8b"},"source":["import time\n","\n","train_dataset = Dataset(train_data, param.batch_size)\n","valid_dataset = batchify(valid_data, param.batch_size)\n","test_dataset = batchify(test_data, 1)\n","\n","start_time = time.time()\n","best_valid_loss = float('inf')\n","\n","train_losses = []\n","optimizer = optim.Adam(model.parameters(), lr=param.lr)\n","\n","for i, batch in enumerate(train_dataset):\n","    optimizer.zero_grad()\n","    train_loss = train_batch(batch, model)\n","    train_losses.append(train_loss.item())\n","    train_loss.backward()\n","    optimizer.step()\n","\n","    if i % 500 == 0:\n","        valid_loss = eval_batches(valid_dataset, model)\n","        avg_train_loss = np.mean(train_losses)\n","        elapsed_time = time.time() - start_time\n","        print(f'iter: {i:06}\\ttrain_loss: {avg_train_loss:.4f}\\tvalid_loss: {valid_loss:.4f}\\telapsed_time: {elapsed_time:.6f}')\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'model_7.w')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["iter: 000000\ttrain_loss: 14.0277\tvalid_loss: 10.5289\telapsed_time: 13.004163\n","iter: 000500\ttrain_loss: 6.8063\tvalid_loss: 6.2893\telapsed_time: 253.597062\n","iter: 001000\ttrain_loss: 6.5011\tvalid_loss: 5.9967\telapsed_time: 495.354018\n","iter: 001500\ttrain_loss: 6.3298\tvalid_loss: 5.8253\telapsed_time: 735.711740\n","iter: 002000\ttrain_loss: 6.2164\tvalid_loss: 5.7266\telapsed_time: 976.529805\n","iter: 002500\ttrain_loss: 6.1326\tvalid_loss: 5.6240\telapsed_time: 1219.047491\n","iter: 003000\ttrain_loss: 6.0572\tvalid_loss: 5.5612\telapsed_time: 1460.310432\n","iter: 003500\ttrain_loss: 5.9941\tvalid_loss: 5.4996\telapsed_time: 1702.529197\n","iter: 004000\ttrain_loss: 5.9422\tvalid_loss: 5.4509\telapsed_time: 1944.225616\n","iter: 004500\ttrain_loss: 5.8952\tvalid_loss: 5.4143\telapsed_time: 2184.745651\n","iter: 005000\ttrain_loss: 5.8504\tvalid_loss: 5.3991\telapsed_time: 2425.976974\n","iter: 005500\ttrain_loss: 5.7446\tvalid_loss: 5.4357\telapsed_time: 2640.832815\n","iter: 006000\ttrain_loss: 5.6974\tvalid_loss: 5.3753\telapsed_time: 2864.414781\n","iter: 006500\ttrain_loss: 5.6611\tvalid_loss: 5.3317\telapsed_time: 3106.334813\n","iter: 007000\ttrain_loss: 5.6263\tvalid_loss: 5.3177\telapsed_time: 3347.543423\n","iter: 007500\ttrain_loss: 5.5934\tvalid_loss: 5.2998\telapsed_time: 3587.825946\n","iter: 008000\ttrain_loss: 5.5644\tvalid_loss: 5.2866\telapsed_time: 3829.275434\n","iter: 008500\ttrain_loss: 5.5360\tvalid_loss: 5.2803\telapsed_time: 4070.415720\n","iter: 009000\ttrain_loss: 5.5092\tvalid_loss: 5.2760\telapsed_time: 4313.648835\n","iter: 009500\ttrain_loss: 5.4827\tvalid_loss: 5.2911\telapsed_time: 4554.223434\n","iter: 010000\ttrain_loss: 5.4583\tvalid_loss: 5.2604\telapsed_time: 4794.723046\n","iter: 010500\ttrain_loss: 5.4341\tvalid_loss: 5.2577\telapsed_time: 5036.032866\n","iter: 011000\ttrain_loss: 5.3885\tvalid_loss: 5.2998\telapsed_time: 5262.094678\n","iter: 011500\ttrain_loss: 5.3527\tvalid_loss: 5.3029\telapsed_time: 5475.543891\n","iter: 012000\ttrain_loss: 5.3312\tvalid_loss: 5.2903\telapsed_time: 5717.353262\n","iter: 012500\ttrain_loss: 5.3087\tvalid_loss: 5.2871\telapsed_time: 5957.337021\n","iter: 013000\ttrain_loss: 5.2867\tvalid_loss: 5.2918\telapsed_time: 6197.364654\n","iter: 013500\ttrain_loss: 5.2664\tvalid_loss: 5.2864\telapsed_time: 6437.986703\n","iter: 014000\ttrain_loss: 5.2465\tvalid_loss: 5.2929\telapsed_time: 6678.934353\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-da4a4ce8ec76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"X34Vhr4GpIZy","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1598814900835,"user_tz":-60,"elapsed":6861907,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["model.load_state_dict(torch.load('model_7.w'))\n","infer_batches(test_dataset, model, save=True)"],"execution_count":null,"outputs":[]}]}