{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"model_5.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"o_VZnTokaE9I","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598812727468,"user_tz":-60,"elapsed":647,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","import numpy as np\n","import pandas as pd\n","from itertools import cycle\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","file_path = './cnn_stories_tokenized'\n","gdd.download_file_from_google_drive(file_id='0BzQ6rtO2VN95cmNuc2xwUS1wdEE', \n","                                    dest_path=file_path+'.zip', \n","                                    unzip=True, \n","                                    showsize=True)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"JKpwsh1Ylyf5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1598812744471,"user_tz":-60,"elapsed":17629,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"51a4aa49-49a5-4e31-aeb8-6b2cbfebe31f"},"source":["from os import listdir\n","from tqdm import tqdm\n","\n","def load_data(file_path):\n","    data = []\n","    file_list = listdir(file_path)\n","    for name in tqdm(file_list):\n","        file_name = file_path + '/' + name\n","        doc = load_doc(file_name)\n","        src, trg = split_doc(doc)\n","        data.append({'src': src, 'trg': trg})\n","    return data\n","\n","def load_doc(file_name):\n","    file = open(file_name, encoding='utf-8')\n","    text = file.read()\n","    file.close()\n","    return text\n","\n","def split_doc(doc):\n","    idx = doc.find('@highlight')\n","    src, trg = doc[:idx], doc[idx:].split('@highlight')\n","    trg = [t.strip() for t in trg if len(t) > 0]\n","    return src, trg\n","\n","def clean_sent(sent):\n","    filter_list = ['/', '-LRB-', '-RRB-', '\\n', '`', '\\'\\'', '\"', '--', '...', 'NEW :']\n","    for token in filter_list:\n","        sent = sent.replace(token, ' ')\n","    sent = ' '.join(sent.split())\n","    sent = sent.lower()\n","    return sent\n","\n","def preprocess(file_path, train_size, valid_size, test_size, max_enc_step, max_dec_step):\n","    # load data from file_path\n","    data = load_data(file_path)\n","    preprocessed = []\n","    for ex in data:\n","        src, trg = ex['src'], ex['trg']\n","        src = clean_sent(src)\n","        trg = [clean_sent(t) for t in trg]\n","        # choose the first hightlight as the truth\n","        # choose the first 2 highlights as the trg\n","        if len(trg) > 1:\n","            trg = trg[:2]\n","            trg = ' . '.join(trg)\n","        else:\n","            trg = trg[0]\n","        trg += ' .'\n","        # truncate examples\n","        if len(src) > max_enc_step > 0: \n","            src = src.split()\n","            src = src[:max_enc_step]\n","            src = ' '.join(src)\n","        if len(trg) > max_dec_step > 0: \n","            trg = trg.split()\n","            trg = trg[:max_dec_step]\n","            trg = ' '.join(trg)\n","        preprocessed.append({'src': src, 'trg': trg})\n","    # split data\n","    train_data = preprocessed[:train_size]\n","    valid_data = preprocessed[train_size:train_size+valid_size]\n","    test_data = preprocessed[-test_size:]\n","    print('Number of train examples: {}'.format(len(train_data)))\n","    print('Number of valid examples: {}'.format(len(valid_data)))\n","    print('Number of test examples: {}'.format(len(test_data)))\n","    return train_data, valid_data, test_data\n","\n","\n","train_size = 90000\n","valid_size = 1579\n","test_size = 1000\n","max_enc_step = 300\n","max_dec_step = -1\n","train_data, valid_data, test_data = preprocess(file_path, \n","                                               train_size, \n","                                               valid_size, \n","                                               test_size,\n","                                               max_enc_step, \n","                                               max_dec_step)\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 92579/92579 [00:03<00:00, 30448.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Number of train examples: 90000\n","Number of valid examples: 1579\n","Number of test examples: 1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zVKkiNGIIArh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1598812781150,"user_tz":-60,"elapsed":54271,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"229805ae-7e70-472b-f1af-03b319ab40af"},"source":["class Vocab():\n","   \n","    def __init__(self, data, max_size=80000, min_freq=4):\n","        self.PAD = 0\n","        self.SOS = 1\n","        self.EOS = 2\n","        self.UNK = 3\n","        self.index2token = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n","        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n","        self.token2count = {}\n","        self.n_tokens = 4\n","        self.max_size = max_size\n","        self.min_freq = min_freq\n","\n","        print(\"Read {} example pairs\".format(len(data)))\n","        for ex in data:\n","            self.add_sent(ex['src'])\n","            self.add_sent(ex['trg'])\n","        print('Number of vocab: {}'.format(self.n_tokens))\n","\n","        vocab_count = list(self.token2count.items()) \n","        vocab_count = sorted(vocab_count, key=lambda x: x[1], reverse=True) # Sort by counts (descending)\n","        if max_size-4 < len(vocab_count):\n","            vocab_count = vocab_count[:max_size-4]\n","\n","        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n","        self.token2count = {}\n","        self.index2token = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.n_tokens = 4\n","\n","        for token, count in vocab_count:\n","            if count >= min_freq: \n","                self.token2index[token] = self.n_tokens\n","                self.token2count[token] = count\n","                self.index2token[self.n_tokens] = token\n","                self.n_tokens += 1    \n","        print('Number of vocab left: {}'.format(self.n_tokens))\n","\n","    def add_sent(self, sent):\n","        for token in sent.split():\n","            self.add_token(token)\n","\n","    def add_token(self, token):\n","        if sum([char.isdigit() for char in token]) == 0:\n","            if token not in self.token2index:\n","                self.token2index[token] = self.n_tokens\n","                self.token2count[token] = 1\n","                self.index2token[self.n_tokens] = token\n","                self.n_tokens += 1\n","            else:\n","                self.token2count[token] += 1\n","\n","    def __getitem__(self, item):\n","        if type(item) == str:\n","            return int(self.token2index.get(item, self.UNK)) # Return the index of <UNK> if input word is not in the vocab\n","        elif type(item) == int:\n","            return self.index2token[item]\n","\n","def tokens2indices(tokens):\n","    indices = [vocab[token] for token in tokens]\n","    indices = [vocab.SOS] + indices + [vocab.EOS]\n","    return indices\n","\n","def indices2tokens(indices):\n","    tokens = [vocab[idx] for idx in indices]\n","    return tokens\n","\n","vocab = Vocab(train_data)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Read 90000 example pairs\n","Number of vocab: 197025\n","Number of vocab left: 80000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q69Hn41vVRwd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598812781151,"user_tz":-60,"elapsed":54256,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["class Example():\n","    def __init__(self, ex, max_src_len=100):\n","\n","        self.src_sent = ex['src']\n","        self.trg_sent = ex['trg']\n","        self.src_tokens = self.src_sent.split() \n","        self.trg_tokens = self.trg_sent.split()\n","        self.src = tokens2indices(self.src_tokens) # For train\n","        self.trg = tokens2indices(self.trg_tokens)[:-1] # For train\n","        self.true = tokens2indices(self.trg_tokens)[1:] # For eval\n","        self.src_len = len(self.src)\n","        self.trg_len = len(self.trg)\n","\n","def pad_sent(sent, max_len):\n","    sent += [vocab.PAD]*(max_len - len(sent))\n","    return sent\n","\n","class Batch():\n","    def __init__(self, example_list):\n","        self.batch_size = len(example_list)\n","        self.original_src = [example.src_sent for example in example_list]\n","        self.original_trg = [example.trg_sent for example in example_list]\n","\n","        self.max_src_len = max([example.src_len for example in example_list])\n","        self.max_trg_len = max([example.trg_len for example in example_list])\n","\n","        self.src = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n","        self.src_mask = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n","        self.trg = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For loss calculation\n","        self.src_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n","        self.trg_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n","        self.true = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For eval\n","        \n","        for i, example in enumerate(example_list):\n","\n","            self.src[i] = pad_sent(example.src, self.max_src_len)\n","            self.src_mask[i] = pad_sent([1]*(example.src_len), self.max_src_len)\n","            self.trg[i] = pad_sent(example.trg, self.max_trg_len)\n","            self.src_len[i] = example.src_len \n","            self.trg_len[i] = example.trg_len \n","            self.true[i, :] = pad_sent(example.true, self.max_trg_len)\n","\n","        self.src = torch.from_numpy(self.src).long().to(device)\n","        self.src_mask = torch.from_numpy(self.src_mask).long().to(device)\n","        self.trg = torch.from_numpy(self.trg).long().to(device)\n","        self.src_len = torch.from_numpy(self.src_len).long().to(device)\n","        self.trg_len = torch.from_numpy(self.trg_len).long().to(device)\n","        self.true = torch.from_numpy(self.true).long().to(device)\n","\n","\n","def batchify(data, batch_size):\n","    examples = [Example(ex) for ex in data]\n","    examples = sorted(examples, key=lambda example: example.src_len, reverse=True)\n","    examples = [examples[i:i+batch_size] for i in range(0, len(examples), batch_size)]\n","    examples = [Batch(batch) for batch in examples]\n","    return examples\n","\n","class Dataset():\n","    def __init__(self, data, batch_size):\n","        self.batches = batchify(data, batch_size)\n","    \n","    def process_data(self):\n","        for batch in self.batches:\n","            yield batch\n","        \n","    def get_stream(self):\n","        return cycle(self.process_data())\n","\n","    def __iter__(self):\n","        return self.get_stream()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"ioC8dDUeZYia","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1598812781152,"user_tz":-60,"elapsed":54243,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"c8103c60-ff06-4e48-9ed1-bfa46cefbfaa"},"source":["class Encoder(nn.Module):\n","    def __init__(self, emb_dim, hid_dim):\n","        super(Encoder, self).__init__()\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n","        self.reduce_hidden = nn.Linear(hid_dim*2, hid_dim, bias=False)\n","        self.reduce_cell = nn.Linear(hid_dim*2, hid_dim, bias=False)\n","    def forward(self, embedded, src_len): \n","        packed = pack_padded_sequence(embedded, src_len, batch_first=True) \n","        output, (hidden, cell) = self.lstm(packed)\n","        output, _ = pad_packed_sequence(output, batch_first=True)\n","        hidden = F.relu(self.reduce_hidden(torch.cat((hidden[-2, :, : ], hidden[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n","        cell = F.relu(self.reduce_cell(torch.cat((cell[-2, :, : ], cell[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n","        return output, hidden, cell\n","\n","class Attention(nn.Module):\n","    def __init__(self, hid_dim):\n","        super(Attention, self).__init__()\n","        self.W_h = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n","        self.W_s = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n","        self.v = nn.Linear(2*hid_dim, 1, bias=False)\n","    def forward(self, output, hidden, cell, src_mask):\n","        hidden = torch.cat((hidden, cell), dim=2).permute(1, 0, 2) # [batch_size, src_len, 2*hid_dim]\n","        energy = self.v(torch.tanh(self.W_h(output) + self.W_s(hidden))) # [batch_size, src_len, 2*hid_dim] EQ.11\n","        attn_dist = F.softmax(energy.squeeze(2), dim=1) # [batch_size, src_len]\n","        # Masking\n","        attn_dist = attn_dist * src_mask # [batch_size, src_len]\n","        attn_dist = attn_dist / attn_dist.sum(1, keepdim=True)\n","        context = torch.bmm(attn_dist.unsqueeze(1), output).squeeze(1) # [batch_size, 2*hid_dim]\n","        return attn_dist, context\n","\n","class Generator(nn.Module):\n","    def __init__(self, emb_dim, hid_dim, vocab_dim):\n","        super(Generator, self).__init__()\n","        self.V1 = nn.Linear(emb_dim+4*hid_dim, hid_dim, bias=True)\n","        self.V2 = nn.Linear(hid_dim, vocab_dim, bias=True)\n","    def forward(self, embedded, hidden, cell, context):\n","        hidden = torch.cat((hidden, cell), dim=2).squeeze(0) # [batch_size, 2*hid_dim]\n","        vocab_dist = F.softmax(self.V2(self.V1(torch.cat((embedded, hidden, context), dim=1))), dim=1) # [batch_size, vocab_dim]\n","        return vocab_dist\n","\n","class Decoder(nn.Module):\n","    def __init__(self, vocab_dim, emb_dim, hid_dim):\n","        super(Decoder, self).__init__()\n","        self.attention = Attention(hid_dim)\n","        self.lstm = nn.LSTM(emb_dim+2*hid_dim, hid_dim, bidirectional=False, batch_first=True)\n","        self.generator = Generator(emb_dim, hid_dim, vocab_dim)\n","    def forward(self, src_mask, embedded, hidden, cell, output):\n","        _, context = self.attention(output, hidden, cell, src_mask)\n","        decoder_input = torch.cat((embedded, context), dim=1).unsqueeze(1) # [batch_size, 1, emb_dim+2*hid_dim]\n","        _, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n","        vocab_dist = self.generator(embedded, hidden, cell, context) # [batch_size, vocab_dim+oov_len]\n","        return vocab_dist, hidden, cell\n","\n","class Param():\n","    def __init__(self):\n","        self.vocab_dim = vocab.n_tokens\n","        self.emb_dim = 300\n","        self.hid_dim = 300\n","        self.lr = 0.001\n","        self.n_iters = 100000\n","        self.batch_size = 16\n","\n","class Model(nn.Module):\n","    def __init__(self, param):\n","        super(Model, self).__init__()\n","        self.embedding = nn.Embedding(param.vocab_dim, param.emb_dim).to(device)\n","        self.encoder = Encoder(param.emb_dim, param.hid_dim).to(device)\n","        self.decoder = Decoder(param.vocab_dim, param.emb_dim, param.hid_dim).to(device)\n","\n","param = Param()\n","model = Model(param)\n","model"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (embedding): Embedding(80000, 300)\n","  (encoder): Encoder(\n","    (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n","    (reduce_hidden): Linear(in_features=600, out_features=300, bias=False)\n","    (reduce_cell): Linear(in_features=600, out_features=300, bias=False)\n","  )\n","  (decoder): Decoder(\n","    (attention): Attention(\n","      (W_h): Linear(in_features=600, out_features=600, bias=False)\n","      (W_s): Linear(in_features=600, out_features=600, bias=False)\n","      (v): Linear(in_features=600, out_features=1, bias=False)\n","    )\n","    (lstm): LSTM(900, 300, batch_first=True)\n","    (generator): Generator(\n","      (V1): Linear(in_features=1500, out_features=300, bias=True)\n","      (V2): Linear(in_features=300, out_features=80000, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"BrRHEUROV0UN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598812781153,"user_tz":-60,"elapsed":54228,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["def calc_loss(vocab_dist, trg_step):\n","    gold_probs = torch.gather(vocab_dist, 1, trg_step.unsqueeze(1)).squeeze()\n","    step_loss = -torch.log(gold_probs + 1e-9) \n","    return step_loss\n","\n","def calc_rouge(refs, cans):\n","    rouge_1 = 0\n","    rouge_2 = 0\n","    rouge_3 = 0\n","    count = 0\n","    for ref, can in zip(refs, cans):\n","        assert type(ref) == type(can) == str\n","        ref = ref.split()\n","        can = can.split()\n","        \n","        rouge_1 += rouge_n([[ref]], [can], 1)\n","        rouge_2 += rouge_n([[ref]], [can], 2)\n","        rouge_3 += rouge_n([[ref]], [can], 3)\n","        count += 1\n","    \n","    rouge_1 = rouge_1*100 / count\n","    rouge_2 = rouge_2*100 / count\n","    rouge_3 = rouge_3*100 / count\n","\n","    return rouge_1, rouge_2, rouge_3\n","\n","def get_ngram(lst, n):\n","    ngram = [tuple(lst[i:i+n]) for i in range(len(lst) - n + 1)]\n","    return ngram\n","\n","def divide(numerator, denominator):\n","    if np.min(denominator) > 0:\n","        return numerator / denominator\n","    else:\n","        return 0.0\n","\n","def match(ref, sys):\n","    intersection = set(ref) & set(sys)\n","    return len(intersection)\n","\n","def single_rouge_n(ref, sys, N):\n","    ref_iterator = get_ngram(ref, N)\n","    sys_iterator = get_ngram(sys, N)\n","    match_count = match(ref_iterator, sys_iterator)\n","    rec_count = len(ref_iterator)\n","    \n","    return match_count, rec_count\n","    \n","def multi_rouge_n(refs, sys, N):\n","    temp_rouge = np.zeros([len(refs), 2])\n","    for i in range(len(refs)):\n","        temp_rouge[i, :] = single_rouge_n(refs[i], sys, N)\n","    rogue = divide(temp_rouge[:, 0], temp_rouge[:, 1])\n","    ind = np.argmax(rogue)\n","    \n","    match_count, rec_count = temp_rouge[ind, ]\n","    return match_count, rec_count\n","    \n","\n","def rouge_n(reference_summaries, candidate_summaries, N):\n","    # https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.XuWlF0VKhPa\n","    match_counts = 0.0\n","    rec_counts = 0.0\n","    \n","    for (refs, can) in zip(reference_summaries, candidate_summaries):\n","        match_count, rec_count = multi_rouge_n(refs, can, N)\n","        match_counts += match_count\n","        rec_counts += rec_count\n","        \n","    score = divide(match_counts, rec_counts)\n","    return score"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"WV3DVqnLfeRa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598812781155,"user_tz":-60,"elapsed":54207,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}}},"source":["def train_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    true = batch.true\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","\n","    embedded = model.embedding(src)\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","\n","    step_losses = []\n","    for t in range(trg.size(1)):\n","        embedded = model.embedding(trg[:, t]) # [batch_size, trg_len, emb_dim]\n","        vocab_dist, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output)\n","        step_loss = calc_loss(vocab_dist, true[:, t])\n","        step_losses.append(step_loss)\n","    step_losses = torch.sum(torch.stack(step_losses, dim=1), dim=1)\n","    batch_loss = torch.mean(step_losses/trg_len)\n","    return batch_loss\n","\n","def eval_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    true = batch.true\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","\n","    embedded = model.embedding(src)\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","\n","    step_losses = []\n","    for t in range(trg.size(1)):\n","        embedded = model.embedding(trg[:, t]) # [batch_size, trg_len, emb_dim]\n","        vocab_dist, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output)\n","        step_loss = calc_loss(vocab_dist, true[:, t])\n","        step_losses.append(step_loss)\n","    step_losses = torch.sum(torch.stack(step_losses, dim=1), dim=1)\n","    batch_loss = torch.mean(step_losses/trg_len)\n","    return batch_loss\n","\n","def eval_batches(batches, model):\n","    valid_losses = []\n","    with torch.no_grad():\n","        for batch in iter(batches):\n","            valid_loss = eval_batch(batch, model)\n","            valid_losses.append(valid_loss.item())\n","        return np.mean(valid_losses)\n","\n","def infer_batch(batch, model):\n","    src = batch.src\n","    src_mask = batch.src_mask\n","    trg = batch.trg\n","    true = batch.true\n","    src_len = batch.src_len\n","    trg_len = batch.trg_len\n","\n","    embedded = model.embedding(src) # [batch_size, src_len, emb_dim]\n","    output, hidden, cell = model.encoder(embedded, src_len)\n","    pred_trgs = []\n","    pred_trg = torch.tensor([vocab.SOS]).to(device) # initial decoder input [batch_size]\n","    for t in range(100):\n","        embedded = model.embedding(pred_trg) # [batch_size, trg_len, emb_dim]\n","        word_dist, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output)\n","        pred_trg = word_dist.argmax(1) # [batch_size]\n","        pred_idx = pred_trg.item()\n","        pred_trgs.append(pred_idx)\n","        if pred_trgs[-1] == vocab.EOS:\n","            break\n","\n","    pred_trgs = indices2tokens(pred_trgs)\n","    pred_trgs = ' '.join(pred_trgs)\n","    return pred_trgs\n","\n","def infer_batches(batches, model, save=False):\n","    refs = []\n","    cans = []\n","    srcs = []\n","    with torch.no_grad():\n","        for batch in batches:\n","            src = batch.original_src[0]\n","            ref = batch.original_trg[0]\n","            can = infer_batch(batch, model)\n","            if can.split()[-1] == vocab[vocab.EOS]:\n","                can = can.split()\n","                can = can[:-1]\n","                can = ' '.join(can)\n","            refs.append(ref)\n","            cans.append(can)\n","            srcs.append(src)\n","    score, _, _ = calc_rouge(refs, cans)\n","    if save == True:\n","        df = pd.DataFrame({'src': srcs, 'reference': refs, 'candidate': cans})\n","        df.to_csv(path_or_buf='model_5.csv', index=False)\n","    return score"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZ5YAmKlE0hh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1598812907238,"user_tz":-60,"elapsed":737,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"e0ad5f31-87ff-4d88-a483-369d9be3d235"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'Embedding has {count_parameters(model.embedding):,} trainable parameters')\n","print(f'Encoder has {count_parameters(model.encoder):,} trainable parameters')\n","print(f'Decoder has {count_parameters(model.decoder):,} trainable parameters')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Embedding has 24,000,000 trainable parameters\n","Encoder has 1,804,800 trainable parameters\n","Decoder has 26,693,300 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xbGuJxHn-XQS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"error","timestamp":1598794813739,"user_tz":-60,"elapsed":635488,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"8cc1e717-39bd-4d04-a814-d55a37f653a5"},"source":["import time\n","\n","train_dataset = Dataset(train_data, param.batch_size)\n","valid_dataset = batchify(valid_data, param.batch_size)\n","test_dataset = batchify(test_data, 1)\n","\n","start_time = time.time()\n","best_valid_loss = float('inf')\n","\n","train_losses = []\n","optimizer = optim.Adam(model.parameters(), lr=param.lr)\n","\n","for i, batch in enumerate(train_dataset):\n","    optimizer.zero_grad()\n","    train_loss = train_batch(batch, model)\n","    train_losses.append(train_loss.item())\n","    train_loss.backward()\n","    optimizer.step()\n","\n","    if i % 500 == 0:\n","        valid_loss = eval_batches(valid_dataset, model)\n","        avg_train_loss = np.mean(train_losses)\n","        elapsed_time = time.time() - start_time\n","        print(f'iter: {i:06}\\ttrain_loss: {avg_train_loss:.4f}\\tvalid_loss: {valid_loss:.4f}\\telapsed_time: {elapsed_time:.6f}')\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'model_5.w')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["iter: 000000\ttrain_loss: 16.8967\tvalid_loss: 13.9366\telapsed_time: 29.314640\n","iter: 000500\ttrain_loss: 6.9532\tvalid_loss: 6.3602\telapsed_time: 547.249378\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-ab00bbbff1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"X34Vhr4GpIZy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598794965708,"user_tz":-60,"elapsed":46364,"user":{"displayName":"Hao Lo","photoUrl":"https://lh5.googleusercontent.com/-rCXD8WsZt0E/AAAAAAAAAAI/AAAAAAAAAHI/VYq1DprMwHw/s64/photo.jpg","userId":"06038609632130697878"}},"outputId":"23a86121-03dd-43d6-f9de-0d8c5322134f"},"source":["model.load_state_dict(torch.load('model_5.w'))\n","infer_batches(test_dataset, model, save=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.667198115591681"]},"metadata":{"tags":[]},"execution_count":25}]}]}