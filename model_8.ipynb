{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "rl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_VZnTokaE9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import cycle\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "file_path = './cnn_stories_tokenized'\n",
        "gdd.download_file_from_google_drive(file_id='0BzQ6rtO2VN95cmNuc2xwUS1wdEE', \n",
        "                                    dest_path=file_path+'.zip', \n",
        "                                    unzip=True, \n",
        "                                    showsize=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKpwsh1Ylyf5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "4bde027c-6537-488f-b021-0a3881a93a97"
      },
      "source": [
        "from os import listdir\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = []\n",
        "    file_list = listdir(file_path)\n",
        "    for name in tqdm(file_list):\n",
        "        file_name = file_path + '/' + name\n",
        "        doc = load_doc(file_name)\n",
        "        src, trg = split_doc(doc)\n",
        "        data.append({'src': src, 'trg': trg})\n",
        "    return data\n",
        "\n",
        "def load_doc(file_name):\n",
        "    file = open(file_name, encoding='utf-8')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def split_doc(doc):\n",
        "    idx = doc.find('@highlight')\n",
        "    src, trg = doc[:idx], doc[idx:].split('@highlight')\n",
        "    trg = [t.strip() for t in trg if len(t) > 0]\n",
        "    return src, trg\n",
        "\n",
        "def clean_sent(sent):\n",
        "    filter_list = ['/', '-LRB-', '-RRB-', '\\n', '`', '\\'\\'', '\"', '--', '...', 'NEW :']\n",
        "    for token in filter_list:\n",
        "        sent = sent.replace(token, ' ')\n",
        "    sent = ' '.join(sent.split())\n",
        "    sent = sent.lower()\n",
        "    return sent\n",
        "\n",
        "def preprocess(file_path, train_size, valid_size, test_size, max_enc_step, max_dec_step):\n",
        "    # load data from file_path\n",
        "    data = load_data(file_path)\n",
        "    preprocessed = []\n",
        "    for ex in data:\n",
        "        src, trg = ex['src'], ex['trg']\n",
        "        src = clean_sent(src)\n",
        "        trg = [clean_sent(t) for t in trg]\n",
        "        # choose the first hightlight as the truth\n",
        "        # choose the first 2 highlights as the trg\n",
        "        if len(trg) > 1:\n",
        "            trg = trg[:2]\n",
        "            trg = ' . '.join(trg)\n",
        "        else:\n",
        "            trg = trg[0]\n",
        "        trg += ' .'\n",
        "        # truncate examples\n",
        "        if len(src) > max_enc_step > 0: \n",
        "            src = src.split()\n",
        "            src = src[:max_enc_step]\n",
        "            src = ' '.join(src)\n",
        "        if len(trg) > max_dec_step > 0: \n",
        "            trg = trg.split()\n",
        "            trg = trg[:max_dec_step]\n",
        "            trg = ' '.join(trg)\n",
        "        preprocessed.append({'src': src, 'trg': trg})\n",
        "    # split data\n",
        "    train_data = preprocessed[:train_size]\n",
        "    valid_data = preprocessed[train_size:train_size+valid_size]\n",
        "    test_data = preprocessed[-test_size:]\n",
        "    print('Number of train examples: {}'.format(len(train_data)))\n",
        "    print('Number of valid examples: {}'.format(len(valid_data)))\n",
        "    print('Number of test examples: {}'.format(len(test_data)))\n",
        "    return train_data, valid_data, test_data\n",
        "\n",
        "\n",
        "train_size = 90000\n",
        "valid_size = 1579\n",
        "test_size = 1000\n",
        "max_enc_step = 300\n",
        "max_dec_step = -1\n",
        "train_data, valid_data, test_data = preprocess(file_path, \n",
        "                                               train_size, \n",
        "                                               valid_size, \n",
        "                                               test_size,\n",
        "                                               max_enc_step, \n",
        "                                               max_dec_step)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 92579/92579 [00:02<00:00, 32272.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of train examples: 90000\n",
            "Number of valid examples: 1579\n",
            "Number of test examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVKkiNGIIArh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5b29413b-aba5-4ddf-a6e4-b7fbe298d135"
      },
      "source": [
        "class Vocab():\n",
        "   \n",
        "    def __init__(self, data, max_size=80000, min_freq=4):\n",
        "        self.PAD = 0\n",
        "        self.SOS = 1\n",
        "        self.EOS = 2\n",
        "        self.UNK = 3\n",
        "        self.index2token = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.token2count = {}\n",
        "        self.n_tokens = 4\n",
        "        self.max_size = max_size\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "        print(\"Read {} example pairs\".format(len(data)))\n",
        "        for ex in data:\n",
        "            self.add_sent(ex['src'])\n",
        "            self.add_sent(ex['trg'])\n",
        "        print('Number of vocab: {}'.format(self.n_tokens))\n",
        "\n",
        "        vocab_count = list(self.token2count.items()) \n",
        "        vocab_count = sorted(vocab_count, key=lambda x: x[1], reverse=True) # Sort by counts (descending)\n",
        "        if max_size-4 < len(vocab_count):\n",
        "            vocab_count = vocab_count[:max_size-4]\n",
        "\n",
        "        self.token2index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "        self.token2count = {}\n",
        "        self.index2token = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.n_tokens = 4\n",
        "\n",
        "        for token, count in vocab_count:\n",
        "            if count >= min_freq: \n",
        "                self.token2index[token] = self.n_tokens\n",
        "                self.token2count[token] = count\n",
        "                self.index2token[self.n_tokens] = token\n",
        "                self.n_tokens += 1    \n",
        "        print('Number of vocab left: {}'.format(self.n_tokens))\n",
        "\n",
        "    def add_sent(self, sent):\n",
        "        for token in sent.split():\n",
        "            self.add_token(token)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if sum([char.isdigit() for char in token]) == 0:\n",
        "            if token not in self.token2index:\n",
        "                self.token2index[token] = self.n_tokens\n",
        "                self.token2count[token] = 1\n",
        "                self.index2token[self.n_tokens] = token\n",
        "                self.n_tokens += 1\n",
        "            else:\n",
        "                self.token2count[token] += 1\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        if type(item) == str:\n",
        "            return int(self.token2index.get(item, self.UNK)) # Return the index of <UNK> if input word is not in the vocab\n",
        "        elif type(item) == int:\n",
        "            return self.index2token[item]\n",
        "\n",
        "def build_oovs(src_tokens):\n",
        "    oovs = [token for token in src_tokens if vocab[token] == vocab.UNK]\n",
        "    return oovs\n",
        "\n",
        "def tokens2indices(tokens):\n",
        "    indices = [vocab[token] for token in tokens]\n",
        "    indices = [vocab.SOS] + indices + [vocab.EOS]\n",
        "    return indices\n",
        "\n",
        "def extended2indices(extended, oovs):\n",
        "    indices = []\n",
        "    for token in extended:\n",
        "        idx = vocab[token]\n",
        "        if idx == vocab.UNK:\n",
        "            idx = vocab.n_tokens + oovs.index(token) if token in oovs else vocab.UNK\n",
        "        indices.append(idx)\n",
        "    indices = [vocab.SOS] + indices + [vocab.EOS]\n",
        "    return indices\n",
        "\n",
        "def indices2extended(indices, oovs):\n",
        "    tokens = []\n",
        "    for idx in indices:\n",
        "        idx = idx.item()\n",
        "        if idx > vocab.n_tokens-1:\n",
        "            token = oovs[idx - vocab.n_tokens]\n",
        "        else:\n",
        "            token = vocab[idx]\n",
        "        tokens.append(token)\n",
        "    return tokens\n",
        "\n",
        "vocab = Vocab(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 90000 example pairs\n",
            "Number of vocab: 197195\n",
            "Number of vocab left: 80000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q69Hn41vVRwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Example():\n",
        "    def __init__(self, ex, max_src_len=100):\n",
        "\n",
        "        self.src_sent = ex['src']\n",
        "        self.trg_sent = ex['trg']\n",
        "        self.src_tokens = self.src_sent.split() \n",
        "        self.trg_tokens = self.trg_sent.split()\n",
        "        self.src = tokens2indices(self.src_tokens) # For train\n",
        "        self.trg = tokens2indices(self.trg_tokens)[:-1] # For train\n",
        "        self.oovs = build_oovs(self.src_tokens)\n",
        "        self.src_extended_vocab = extended2indices(self.src_tokens, self.oovs) # For attn dist\n",
        "        self.trg_extended_vocab = extended2indices(self.trg_tokens, self.oovs)[1:] # For eval\n",
        "        self.src_len = len(self.src)\n",
        "        self.trg_len = len(self.trg)\n",
        "\n",
        "def pad_sent(sent, max_len):\n",
        "    sent += [vocab.PAD]*(max_len - len(sent))\n",
        "    return sent\n",
        "\n",
        "class Batch():\n",
        "    def __init__(self, example_list):\n",
        "        self.batch_size = len(example_list)\n",
        "        self.original_src = [example.src_sent for example in example_list]\n",
        "        self.original_trg = [example.trg_sent for example in example_list]\n",
        "\n",
        "        self.max_src_len = max([example.src_len for example in example_list])\n",
        "        self.max_trg_len = max([example.trg_len for example in example_list])\n",
        "\n",
        "        self.src = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n",
        "        self.src_mask = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For train\n",
        "        self.trg = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For loss calculation\n",
        "        self.src_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n",
        "        self.trg_len = np.zeros([self.batch_size], dtype=np.float32) # For train\n",
        "        self.src_extended_vocab = np.zeros([self.batch_size, self.max_src_len], dtype=np.int32) # For attn dist\n",
        "        self.trg_extended_vocab = np.zeros([self.batch_size, self.max_trg_len], dtype=np.int32) # For eval\n",
        "        self.max_oovs_len = max([len(example.oovs) for example in example_list]) # For attn dist\n",
        "        self.oovs = [example.oovs for example in example_list] # For attn dist\n",
        "        \n",
        "        for i, example in enumerate(example_list):\n",
        "\n",
        "            self.src[i] = pad_sent(example.src, self.max_src_len)\n",
        "            self.src_mask[i] = pad_sent([1]*(example.src_len), self.max_src_len)\n",
        "            self.trg[i] = pad_sent(example.trg, self.max_trg_len)\n",
        "            self.src_len[i] = example.src_len \n",
        "            self.trg_len[i] = example.trg_len \n",
        "            self.src_extended_vocab[i, :] = pad_sent(example.src_extended_vocab, self.max_src_len)\n",
        "            self.trg_extended_vocab[i, :] = pad_sent(example.trg_extended_vocab, self.max_trg_len)\n",
        "\n",
        "        self.src = torch.from_numpy(self.src).long().to(device)\n",
        "        self.src_mask = torch.from_numpy(self.src_mask).long().to(device)\n",
        "        self.trg = torch.from_numpy(self.trg).long().to(device)\n",
        "        self.src_len = torch.from_numpy(self.src_len).long().to(device)\n",
        "        self.trg_len = torch.from_numpy(self.trg_len).long().to(device)\n",
        "        self.src_extended_vocab = torch.from_numpy(self.src_extended_vocab).long().to(device)\n",
        "        self.trg_extended_vocab = torch.from_numpy(self.trg_extended_vocab).long().to(device)\n",
        "        self.oov_pad = torch.zeros(self.batch_size, self.max_oovs_len).to(device) if self.max_oovs_len > 0 else None # [batch_size, oov_len]\n",
        "\n",
        "\n",
        "def batchify(data, batch_size):\n",
        "    examples = [Example(ex) for ex in data]\n",
        "    examples = sorted(examples, key=lambda example: example.src_len, reverse=True)\n",
        "    examples = [examples[i:i+batch_size] for i in range(0, len(examples), batch_size)]\n",
        "    examples = [Batch(batch) for batch in examples]\n",
        "    return examples\n",
        "\n",
        "class Dataset():\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.batches = batchify(data, batch_size)\n",
        "    \n",
        "    def process_data(self):\n",
        "        for batch in self.batches:\n",
        "            yield batch\n",
        "        \n",
        "    def get_stream(self):\n",
        "        return cycle(self.process_data())\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self.get_stream()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioC8dDUeZYia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "5c1eb886-5b8a-4809-dc8c-d156e4343756"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, emb_dim, hid_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
        "        self.reduce_hidden = nn.Linear(hid_dim*2, hid_dim, bias=False)\n",
        "        self.reduce_cell = nn.Linear(hid_dim*2, hid_dim, bias=False)\n",
        "    def forward(self, embedded, src_len): \n",
        "        packed = pack_padded_sequence(embedded, src_len, batch_first=True) \n",
        "        output, (hidden, cell) = self.lstm(packed)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        hidden = F.relu(self.reduce_hidden(torch.cat((hidden[-2, :, : ], hidden[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n",
        "        cell = F.relu(self.reduce_cell(torch.cat((cell[-2, :, : ], cell[-1, :, : ]), dim=1))).unsqueeze(0) # [1, batch_size, hid_dim]\n",
        "        return output, hidden, cell\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W_h = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n",
        "        self.W_s = nn.Linear(2*hid_dim, 2*hid_dim, bias=False)\n",
        "        self.v = nn.Linear(2*hid_dim, 1, bias=False)\n",
        "    def forward(self, output, hidden, cell, src_mask):\n",
        "        hidden = torch.cat((hidden, cell), dim=2).permute(1, 0, 2) # [batch_size, src_len, 2*hid_dim]\n",
        "        energy = self.v(torch.tanh(self.W_h(output) + self.W_s(hidden))) # [batch_size, src_len, 2*hid_dim] EQ.11\n",
        "        attn_dist = F.softmax(energy.squeeze(2), dim=1) # [batch_size, src_len]\n",
        "        # Masking\n",
        "        attn_dist = attn_dist * src_mask # [batch_size, src_len]\n",
        "        attn_dist = attn_dist / attn_dist.sum(1, keepdim=True)\n",
        "        context = torch.bmm(attn_dist.unsqueeze(1), output).squeeze(1) # [batch_size, 2*hid_dim]\n",
        "        return attn_dist, context\n",
        "\n",
        "class Pointer_Generator(nn.Module):\n",
        "    def __init__(self, emb_dim, hid_dim, output_dim):\n",
        "        super(Pointer_Generator, self).__init__()\n",
        "        self.ptr = nn.Linear(emb_dim+4*hid_dim, 1, bias=True)\n",
        "        self.V1 = nn.Linear(emb_dim+4*hid_dim, hid_dim, bias=True)\n",
        "        self.V2 = nn.Linear(hid_dim, output_dim, bias=True)\n",
        "    def forward(self, embedded, hidden, cell, context, attn_dist, src_extended_vocab, oov_pad):\n",
        "\n",
        "        hidden = torch.cat((hidden, cell), dim=2).squeeze(0) # [batch_size, 2*hid_dim]\n",
        "\n",
        "        # Generation probability\n",
        "        gen_prob = torch.sigmoid(self.ptr(torch.cat((embedded, hidden, context), dim=1))) # [batch_size, 1]\n",
        "\n",
        "        # Vocabulary distribution \n",
        "        vocab_dist = F.softmax(self.V2(self.V1(torch.cat((embedded, hidden, context), dim=1))), dim=1) # [batch_size, output_dim]\n",
        "        vocab_dist = gen_prob * vocab_dist # [batch_size, output_dim+oov_len]\n",
        "        vocab_dist = torch.cat((vocab_dist, oov_pad), dim=1) if oov_pad != None else vocab_dist\n",
        "            \n",
        "        # Attention distribution\n",
        "        attn_dist = (1-gen_prob) * attn_dist # [batch_size, src_len]\n",
        "\n",
        "        # Word distribution\n",
        "        word_dist = vocab_dist.scatter_add(1, src_extended_vocab, attn_dist) # [batch_size, output_dim+oov_len]\n",
        "        return word_dist\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention = Attention(hid_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim+2*hid_dim, hid_dim, bidirectional=False, batch_first=True)\n",
        "        self.pointer_generator = Pointer_Generator(emb_dim, hid_dim, output_dim)\n",
        "    def forward(self, src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad):\n",
        "        attn_dist, context = self.attention(output, hidden, cell, src_mask)\n",
        "        decoder_input = torch.cat((embedded, context), dim=1).unsqueeze(1) # [batch_size, 1, emb_dim+2*hid_dim]\n",
        "        _, (hidden, cell) = self.lstm(decoder_input, (hidden, cell))\n",
        "        word_dist = self.pointer_generator(embedded, hidden, cell, context, attn_dist, src_extended_vocab, oov_pad) # [batch_size, output_dim+oov_len]\n",
        "        return word_dist, hidden, cell\n",
        "\n",
        "class Param():\n",
        "    def __init__(self):\n",
        "        self.vocab_dim = vocab.n_tokens\n",
        "        self.emb_dim = 300\n",
        "        self.hid_dim = 300\n",
        "        self.lr = 0.001\n",
        "        self.n_iters = 100000\n",
        "        self.batch_size = 16\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, param):\n",
        "        super(Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(param.vocab_dim, param.emb_dim).to(device)\n",
        "        self.encoder = Encoder(param.emb_dim, param.hid_dim).to(device)\n",
        "        self.decoder = Decoder(param.vocab_dim, param.emb_dim, param.hid_dim).to(device)\n",
        "\n",
        "param = Param()\n",
        "model = Model(param)\n",
        "model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embedding): Embedding(80000, 300)\n",
              "  (encoder): Encoder(\n",
              "    (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
              "    (reduce_hidden): Linear(in_features=600, out_features=300, bias=False)\n",
              "    (reduce_cell): Linear(in_features=600, out_features=300, bias=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (W_h): Linear(in_features=600, out_features=600, bias=False)\n",
              "      (W_s): Linear(in_features=600, out_features=600, bias=False)\n",
              "      (v): Linear(in_features=600, out_features=1, bias=False)\n",
              "    )\n",
              "    (lstm): LSTM(900, 300, batch_first=True)\n",
              "    (pointer_generator): Pointer_Generator(\n",
              "      (ptr): Linear(in_features=1500, out_features=1, bias=True)\n",
              "      (V1): Linear(in_features=1500, out_features=300, bias=True)\n",
              "      (V2): Linear(in_features=300, out_features=80000, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P4FcNP5R-FK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngram(lst, n):\n",
        "    ngram = [tuple(lst[i:i+n]) for i in range(len(lst) - n + 1)]\n",
        "    return ngram\n",
        "\n",
        "def match(n_gram_1, n_gram_2):\n",
        "    intersection = set(n_gram_1) & set(n_gram_2)\n",
        "    return len(intersection)\n",
        "\n",
        "def single_rouge_n(reference, generated, N=1):\n",
        "    reference = get_ngram(reference.split(), N)\n",
        "    generated = get_ngram(generated.split(), N)\n",
        "    match_count = match(reference, generated)\n",
        "    rec_count = len(reference)\n",
        "    score = match_count / rec_count if rec_count > 0 else 0\n",
        "    return score\n",
        "\n",
        "def calc_rouge(reference_sents, generated_sents, N=1):\n",
        "    scores = []\n",
        "    for (reference, generated) in zip(reference_sents, generated_sents):\n",
        "        score = single_rouge_n(reference, generated, N)\n",
        "        scores.append(score)\n",
        "    return torch.tensor(scores).float().to(device) # [batch_size]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fdy2qjh8nUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(batch, model, gamma):\n",
        "    src = batch.src\n",
        "    src_len = batch.src_len\n",
        "    embedded = model.embedding(src)\n",
        "    state = model.encoder(embedded, src_len)\n",
        "    loss_CE = train_batch_CE(batch, model, state) if gamma < 1 else torch.zeros(1).to(device)\n",
        "    loss_RL = train_batch_RL(batch, model, state) if gamma > 0 else torch.zeros(1).to(device)\n",
        "    loss = gamma*loss_RL + (1-gamma)*loss_CE\n",
        "    return loss\n",
        "\n",
        "def sample_decode(batch, model, state):\n",
        "    # https://pytorch.org/docs/stable/distributions.html\n",
        "    src_mask = batch.src_mask\n",
        "    src_extended_vocab = batch.src_extended_vocab\n",
        "    oov_pad = batch.oov_pad\n",
        "    (output, hidden, cell) = state\n",
        "\n",
        "    actions = []\n",
        "    log_probs = []\n",
        "    eos_masks = []\n",
        "\n",
        "    action = torch.tensor([vocab.SOS]).repeat(batch.batch_size).to(device)\n",
        "    for t in range(100):\n",
        "        embedded = model.embedding(action)\n",
        "        probs, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        log_prob = m.log_prob(action)\n",
        "        action, eos_mask = next_action(action)\n",
        "\n",
        "        actions.append(action)\n",
        "        eos_masks.append(eos_mask)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "    actions = torch.stack(actions, dim=1)\n",
        "    eos_masks = torch.stack(eos_masks, dim=1)\n",
        "    log_probs = torch.stack(log_probs, dim=1)*eos_masks\n",
        "\n",
        "    log_probs_sum = torch.sum(log_probs, dim=1)/torch.sum(eos_masks, dim=1)   \n",
        "    sents = actions2sents(actions, batch)\n",
        "    return sents, log_probs_sum\n",
        "\n",
        "def greedy_decode(batch, model, state):\n",
        "    with torch.no_grad():\n",
        "        src_mask = batch.src_mask\n",
        "        src_extended_vocab = batch.src_extended_vocab\n",
        "        oov_pad = batch.oov_pad\n",
        "        (output, hidden, cell) = state\n",
        "\n",
        "        actions = []\n",
        "        \n",
        "        action = torch.tensor([vocab.SOS]).repeat(batch.batch_size).to(device)\n",
        "        for t in range(100):\n",
        "            embedded = model.embedding(action)\n",
        "            probs, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad)\n",
        "            _, action = torch.max(probs, dim=1)\n",
        "            action, _ = next_action(action)\n",
        "\n",
        "            actions.append(action)\n",
        "\n",
        "        actions = torch.stack(actions, dim=1)\n",
        "        sents = actions2sents(actions, batch)\n",
        "    return sents\n",
        "\n",
        "def actions2sents(actions, batch):\n",
        "    sents = []    \n",
        "    for i in range(batch.batch_size):\n",
        "        sent = list(actions[i].cpu().numpy())\n",
        "        sent = indices2extended(sent, batch.oovs[i])\n",
        "        eos = vocab[vocab.EOS]\n",
        "        sent = sent[:sent.index(eos)] if eos in sent else sent\n",
        "        sent = ' '.join(sent)\n",
        "        sents.append(sent)\n",
        "    return sents\n",
        "\n",
        "def next_action(action):\n",
        "    oov_mask = (action > vocab.n_tokens-1).long()                                  \n",
        "    action = oov_mask*vocab.UNK + (1-oov_mask)*action\n",
        "    eos_mask = torch.ones(action.size(0)).to(device)\n",
        "    eos_mask[action == vocab.EOS] == 0\n",
        "    return action, eos_mask\n",
        "\n",
        "def train_batch_RL(batch, model, state):\n",
        "    sample_sents, log_probs = sample_decode(batch, model, state)\n",
        "    greedy_sents = greedy_decode(batch, model, state)\n",
        "    sample_reward = calc_rouge(batch.original_src, sample_sents)\n",
        "    baseline_reward = calc_rouge(batch.original_src, greedy_sents)\n",
        "    loss_RL = torch.mean((baseline_reward - sample_reward) * log_probs)\n",
        "    return loss_RL\n",
        "\n",
        "def calc_CE_loss(vocab_dist, trg_step):\n",
        "    probs = torch.gather(vocab_dist, 1, trg_step.unsqueeze(1)).squeeze()\n",
        "    step_loss = -torch.log(probs + 1e-9) \n",
        "    return step_loss\n",
        "\n",
        "def train_batch_CE(batch, model, state):\n",
        "    src_mask = batch.src_mask\n",
        "    trg = batch.trg\n",
        "    trg_len = batch.trg_len\n",
        "    src_extended_vocab = batch.src_extended_vocab\n",
        "    trg_extended_vocab = batch.trg_extended_vocab\n",
        "    oov_pad = batch.oov_pad\n",
        "    (output, hidden, cell) = state\n",
        "\n",
        "    step_losses = []\n",
        "    for t in range(trg.size(1)):\n",
        "        embedded = model.embedding(trg[:, t])\n",
        "        word_dist, hidden, cell = model.decoder(src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad)\n",
        "        step_loss = calc_CE_loss(word_dist, trg_extended_vocab[:, t])\n",
        "        step_losses.append(step_loss)\n",
        "    step_losses = torch.sum(torch.stack(step_losses, dim=1), dim=1)\n",
        "    loss_CE = torch.mean(step_losses/trg_len)\n",
        "    return loss_CE"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV3DVqnLfeRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def infer_batch(batch, model):\n",
        "    assert batch.batch_size == 1\n",
        "    src = batch.src\n",
        "    src_len = batch.src_len\n",
        "    embedded = model.embedding(src)\n",
        "    state = model.encoder(embedded, src_len)\n",
        "    sents = greedy_decode(batch, model, state)\n",
        "    return sents\n",
        "\n",
        "def infer_batches(batches, model, save=False, name='rl.csv'):\n",
        "    refs = []\n",
        "    cans = []\n",
        "    srcs = []\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        for batch in batches:\n",
        "            src = batch.original_src[0]\n",
        "            ref = batch.original_trg[0]\n",
        "            can = infer_batch(batch, model)[0]\n",
        "            score = single_rouge_n(ref, can)\n",
        "\n",
        "            refs.append(ref)\n",
        "            cans.append(can)\n",
        "            srcs.append(src)\n",
        "            scores.append(score)\n",
        "\n",
        "    score = np.mean(scores)\n",
        "    if save == True:\n",
        "        df = pd.DataFrame({'src': srcs, 'reference': refs, 'candidate': cans})\n",
        "        df.to_csv(path_or_buf=name, index=False)\n",
        "    return score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ5YAmKlE0hh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3aa4526e-836f-462a-8058-59bfda81b957"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Embedding has {count_parameters(model.embedding):,} trainable parameters')\n",
        "print(f'Encoder has {count_parameters(model.encoder):,} trainable parameters')\n",
        "print(f'Decoder has {count_parameters(model.decoder):,} trainable parameters')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding has 24,000,000 trainable parameters\n",
            "Encoder has 1,804,800 trainable parameters\n",
            "Decoder has 26,694,801 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX3zGjCPYwgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "train_dataset = Dataset(train_data, param.batch_size)\n",
        "valid_dataset = batchify(valid_data, 1)\n",
        "test_dataset = batchify(test_data, 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6wMTqzY5ffu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "a8fdc4e9-0d48-4c36-c6e9-27af129de3de"
      },
      "source": [
        "model.load_state_dict(torch.load('pretrained.w'))\n",
        "infer_batches(test_dataset, model, save=True, name='pretrained.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bece52de05fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained.w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minfer_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pretrained.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-63aed18e8ee3>\u001b[0m in \u001b[0;36minfer_batches\u001b[0;34m(batches, model, save, name)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_trg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mcan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_rouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-63aed18e8ee3>\u001b[0m in \u001b[0;36minfer_batch\u001b[0;34m(batch, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cb95a4269373>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(batch, model, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_extended_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-eb6182348e63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mattn_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 1, emb_dim+2*hid_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mword_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_extended_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_pad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, output_dim+oov_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbGuJxHn-XQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "20714852-4088-4375-9736-b5cee480ab17"
      },
      "source": [
        "model.load_state_dict(torch.load('pretrained.w'))\n",
        "\n",
        "start_time = time.time()\n",
        "best_rouge = 0\n",
        "gamma = 0.9\n",
        "train_losses = []\n",
        "optimizer = optim.Adam(model.parameters(), lr=param.lr)\n",
        "\n",
        "for i, batch in enumerate(train_dataset):\n",
        "    if i % 300 == 0 and i != 0:\n",
        "        gamma += 0.05\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss = train_batch(batch, model, gamma)\n",
        "    train_losses.append(train_loss.item())\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        valid_rouge = infer_batches(valid_dataset, model)\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f'iter: {i:06}\\tgamma: {gamma}\\ttrain_loss: {avg_train_loss:.4f}\\tvalid_rouge: {valid_rouge:.4f}\\telapsed_time: {elapsed_time:.6f}')\n",
        "\n",
        "    if valid_rouge > best_rouge:\n",
        "        best_rouge = valid_rouge\n",
        "        torch.save(model.state_dict(), 'rl.w')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter: 000000\tgamma: 0.9\ttrain_loss: 0.4133\tvalid_rouge: 0.2221\telapsed_time: 312.149242\n",
            "iter: 000100\tgamma: 0.9\ttrain_loss: 0.3615\tvalid_rouge: 0.2312\telapsed_time: 816.794215\n",
            "iter: 000200\tgamma: 0.9\ttrain_loss: 0.3533\tvalid_rouge: 0.2401\telapsed_time: 1320.661373\n",
            "iter: 000300\tgamma: 0.9500000000000001\ttrain_loss: 0.3527\tvalid_rouge: 0.2275\telapsed_time: 1823.751273\n",
            "iter: 000400\tgamma: 0.9500000000000001\ttrain_loss: 0.3073\tvalid_rouge: 0.2345\telapsed_time: 2325.733480\n",
            "iter: 000500\tgamma: 0.9500000000000001\ttrain_loss: 0.2796\tvalid_rouge: 0.2315\telapsed_time: 2828.787404\n",
            "iter: 000600\tgamma: 1.0\ttrain_loss: 0.2603\tvalid_rouge: 0.2393\telapsed_time: 3330.650475\n",
            "iter: 000700\tgamma: 1.0\ttrain_loss: 0.2254\tvalid_rouge: 0.2810\telapsed_time: 3794.255626\n",
            "iter: 000800\tgamma: 1.0\ttrain_loss: 0.2208\tvalid_rouge: 0.2178\telapsed_time: 4259.083356\n",
            "iter: 000900\tgamma: 1.05\ttrain_loss: 0.2194\tvalid_rouge: 0.1936\telapsed_time: 4723.061026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ne() received an invalid combination of arguments - got (NoneType), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (!NoneType!)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8c724b7c665b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mvalid_rouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-63aed18e8ee3>\u001b[0m in \u001b[0;36minfer_batches\u001b[0;34m(batches, model, save, name)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_trg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mcan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msingle_rouge_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-63aed18e8ee3>\u001b[0m in \u001b[0;36minfer_batch\u001b[0;34m(batch, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-cb95a4269373>\u001b[0m in \u001b[0;36mgreedy_decode\u001b[0;34m(batch, model, state)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_extended_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-eb6182348e63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_mask, embedded, hidden, cell, output, src_extended_vocab, oov_pad)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 1, emb_dim+2*hid_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mword_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_extended_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_pad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, output_dim+oov_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-eb6182348e63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embedded, hidden, cell, context, attn_dist, src_extended_vocab, oov_pad)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mvocab_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, output_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvocab_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvocab_dist\u001b[0m \u001b[0;31m# [batch_size, output_dim+oov_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mvocab_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moov_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moov_pad\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvocab_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Attention distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X34Vhr4GpIZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5fb4078d-2128-463b-997b-c753200b9187"
      },
      "source": [
        "model.load_state_dict(torch.load('rl.w'))\n",
        "infer_batches(test_dataset, model, save=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.280294509739343"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}
